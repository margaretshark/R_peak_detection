{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ba22c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78790f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00d81e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywt\n",
    "from collections import Counter \n",
    "from scipy import signal as sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39263b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import wfdb\n",
    "from wfdb import processing\n",
    "import os\n",
    "from functools import partial\n",
    "from utils import plot_signal_with_r_peaks, load_patient, extract_test_windows, mean\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from collections import Counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acec8028",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_unet1d import *\n",
    "import torch_optimizer as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5631c40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = 'mit-bih-arrhythmia' # raw\n",
    "\n",
    "data_dir = 'filtered-mit-bih-arrhythmia' # filtered "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "86fab282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Window size is 7200\n"
     ]
    }
   ],
   "source": [
    "# Window/Segment length \n",
    "\n",
    "l = 20 #seconds\n",
    "# window stride for testing. \n",
    "\n",
    "s = 10 #seconds\n",
    "# Sapmling frequency of ecg signal is 360 hz. \n",
    "# (https://archive.physionet.org/physiobank/database/html/mitdbdir/intro.htm#annotations)\n",
    "\n",
    "fs = 360\n",
    "# Window/Segment length in samples. \n",
    "win_size = l*fs\n",
    "print(f\"Window size is {win_size}\")\n",
    "# Stride for test window in samples. \n",
    "stride = s*fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "497e1b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(ecg):\n",
    "    \n",
    "    signal = ecg.flatten()\n",
    "\n",
    "    # Apply DWT\n",
    "    wavelet = 'db4'\n",
    "    level = 9\n",
    "    coeffs = pywt.wavedec(signal, wavelet, level=level)\n",
    "\n",
    "    # Remove baseline wandering\n",
    "    coeffs[1:] = [pywt.threshold(i, np.std(i)/2) for i in coeffs[1:]]\n",
    "    filtered_signal = pywt.waverec(coeffs, wavelet)\n",
    "\n",
    "    # Apply lowpass filter\n",
    "    nyquist = int(180)\n",
    "    cutoff = 40\n",
    "    b, a = sig.butter(4, cutoff/nyquist, 'low')\n",
    "    filtered_signal = sig.filtfilt(b, a, filtered_signal)\n",
    "    return filtered_signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "557bf0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_training_windows(ecg, win_size, Rpeaks_pos):\n",
    "    \n",
    "    print('Preparing Training Data for R-peaks detection')\n",
    "\n",
    "    # Total windows in ecg. (Number of training examples)\n",
    "    tot_wins = int(len(ecg)/win_size)\n",
    "\n",
    "    X_train = np.zeros((tot_wins,win_size), dtype=np.float64)\n",
    "    y_train = np.zeros((tot_wins,win_size))\n",
    "\n",
    "    # Annotations for each window\n",
    "    R = []\n",
    "    \n",
    "    normalize = partial(processing.normalize_bound, lb=-1, ub=1)\n",
    "    \n",
    "    for i in tqdm(range(tot_wins)):\n",
    "    \n",
    "        # Start of window in whole ecg stream\n",
    "        st = i*win_size\n",
    "        \n",
    "        # End of window\n",
    "        end = st + win_size\n",
    "\n",
    "        # R peaks in the current window\n",
    "        rIndx = np.where((Rpeaks_pos >= st) & (Rpeaks_pos < end))[0]\n",
    "\n",
    "        R.append(Rpeaks_pos[rIndx]-st)\n",
    "\n",
    "                  \n",
    "        for j in Rpeaks_pos[rIndx]:\n",
    "            r = int(j)-st\n",
    "            y_train[i,r-2:r+3] = 1\n",
    "\n",
    "\n",
    "        # If ecg window is non zero. Normalize it. \n",
    "        if ecg[st:end].any():\n",
    "            X_train[i,:] = np.squeeze(np.apply_along_axis(normalize, 0, ecg[st:end]))\n",
    "        # All zero ecg window\n",
    "        else:\n",
    "            X_train[i,:] = ecg[st:end].T\n",
    "\n",
    "\n",
    "    X_train = np.expand_dims(X_train, axis=1)\n",
    "    \n",
    "    y_train = np.expand_dims(y_train, axis=1)\n",
    "    \n",
    "    return X_train, y_train, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "95908902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of patient records is 48 \n"
     ]
    }
   ],
   "source": [
    "patients = [x for x in list(set([x.split('.')[0] for x in os.listdir(f'./{data_dir}/')])) if len(x) > 1]\n",
    "print(f\"The number of patient records is {len(patients)} \")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "856648fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['223', '107', '205', '118', '113', '111', '217', '104', '213',\n",
       "       '114', '112', '117', '222', '212', '208', '221', '119', '202',\n",
       "       '200', '232', '109', '214', '105', '106', '219', '121', '100',\n",
       "       '230', '124', '108', '102', '115', '233', '201', '231', '101',\n",
       "       '207', '210', '234', '123', '122', '203', '209', '220', '228',\n",
       "       '215', '103', '116'], dtype='<U3')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.RandomState(seed=42).permutation(patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "54e493b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define  the splitting strategy for training our detection model\n",
    "def data_split(patients):\n",
    "    SEED = 42\n",
    "    indices = np.random.RandomState(seed=SEED).permutation(patients)\n",
    "    m = len(patients)\n",
    "    training_idx, val_idx, test_idx = indices[:int(m*0.7)], indices[int(m*0.7):int(m*0.85)], indices[int(m*0.85):]\n",
    "\n",
    "    return training_idx, test_idx, val_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3612019c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_idx, test_idx, val_idx = data_split(patients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6ffa655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_testing_windows(ecg, win_size, Rpeaks_pos):\n",
    "    overlap = 2000\n",
    "    hop_size = win_size - overlap\n",
    "    \n",
    "    ecg_pad = np.pad(ecg,(0, 5200), mode='edge')\n",
    "    \n",
    "    print('Preparing testing data for R-peaks detection')\n",
    "    \n",
    "    # Total windows in ecg. (Number of training examples)\n",
    "    tot_wins = int((len(ecg) - overlap)/ hop_size)\n",
    "    \n",
    "    X_train = np.zeros((tot_wins,win_size), dtype=np.float64)\n",
    "    y_train = np.zeros((tot_wins,win_size))\n",
    "\n",
    "    # Annotations for each window\n",
    "    R = []\n",
    "    \n",
    "    normalize = partial(processing.normalize_bound, lb=-1, ub=1)\n",
    "    \n",
    "#     for i in range(0, len(pad_sig), stride):\n",
    "#     for i in range(0, len(tot_wins), stride):\n",
    "\n",
    "    pad_id = np.arange(ecg_pad.shape[0])\n",
    "    \n",
    "    for i in tqdm(range(tot_wins)):\n",
    "    \n",
    "        # Start of window in whole ecg stream\n",
    "        st = i*win_size\n",
    "        \n",
    "        # End of window\n",
    "        end = st + win_size\n",
    "\n",
    "        # R peaks in the current window\n",
    "        rIndx = np.where((Rpeaks_pos >= st) & (Rpeaks_pos < end))[0]\n",
    "\n",
    "        R.append(Rpeaks_pos[rIndx]-st)\n",
    "\n",
    "                  \n",
    "        for j in Rpeaks_pos[rIndx]:\n",
    "            r = int(j)-st\n",
    "            y_train[i,r-2:r+3] = 1\n",
    "\n",
    "\n",
    "        # If ecg window is non zero. Normalize it. \n",
    "        if ecg_pad[st:end].any():\n",
    "            X_train[i,:] = np.squeeze(np.apply_along_axis(normalize, 0, ecg_pad[st:end]))\n",
    "        # All zero ecg window\n",
    "        else:\n",
    "#             print(i)\n",
    "            pass\n",
    "#             X_train[i,:] = ecg_pad[st:end].T\n",
    "\n",
    "\n",
    "    X_train = np.expand_dims(X_train, axis=1)\n",
    "    \n",
    "    y_train = np.expand_dims(y_train, axis=1)\n",
    "    \n",
    "    return X_train, y_train, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0547e0ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def data_laoding(idx, win_size, data_dir): \n",
    "    \n",
    "    for i, pat_num in zip(range(len(idx)), idx):\n",
    "\n",
    "        ecg, Rpeaks_pos = load_patient(data_dir, str(pat_num))\n",
    "\n",
    "        X_train_s, y_train_s, R = extract_training_windows(ecg, win_size, Rpeaks_pos)\n",
    "        if i == 0:\n",
    "            X_train = X_train_s\n",
    "            y_train = y_train_s\n",
    "        else:\n",
    "            X_train = np.concatenate((X_train, X_train_s))\n",
    "            y_train = np.concatenate((y_train, y_train_s))\n",
    "            \n",
    "    return X_train , y_train, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a23ee5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def data_laoding_testing(idx, win_size, data_dir): \n",
    "    \n",
    "#     for i, pat_num in zip(range(len(idx)), idx):\n",
    "\n",
    "#         ecg, Rpeaks_pos = load_patient(data_dir, str(pat_num))\n",
    "        \n",
    "#         X_test_s, y_test_s, R = extract_testing_windows(ecg, win_size, Rpeaks_pos)\n",
    "#         if i == 0:\n",
    "#             X_test = X_test_s\n",
    "#             y_test = y_test_s\n",
    "#         else:\n",
    "#             X_test = np.concatenate((X_test, X_test_s))\n",
    "#             y_test = np.concatenate((y_test, y_test_s))\n",
    "            \n",
    "#     return X_test , X_test, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "890e6f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_dir = 'mit-bih-arrhythmia',\n",
    "                 idx_im = training_idx, label = 'train'):\n",
    "        \n",
    "        self.win_size = 7200\n",
    "        self.idx_im = idx_im\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        if label == 'train':\n",
    "            self.X, self.y, self.R = data_laoding(self.idx_im, self.win_size, self.data_dir)\n",
    "            \n",
    "        if label == 'test':\n",
    "            self.X, self.y, self.R = data_laoding_testing(self.idx_im, self.win_size, self.data_dir)\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.idx_im)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        X_inp = torch.from_numpy(self.X[idx]).float()\n",
    "        y_inp = torch.from_numpy(self.y[idx]).float()\n",
    "        half_pad = (7232 - self.win_size) // 2\n",
    "        p = torch.nn.ConstantPad1d(half_pad, 0)\n",
    "        X_inp1 = p(X_inp)\n",
    "        y_inp1 = p(y_inp)\n",
    "\n",
    "        return X_inp1, y_inp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e5abcde9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 223\n",
      "Total Beats :  2602\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 15274.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 107\n",
      "Total Beats :  60\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 10519.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 205\n",
      "Total Beats :  2658\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 12658.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 118\n",
      "Total Beats :  113\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 19985.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 113\n",
      "Total Beats :  1790\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 17374.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 111\n",
      "Total Beats :  2\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 20477.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 217\n",
      "Total Beats :  473\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 13016.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 104\n",
      "Total Beats :  210\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 21982.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 213\n",
      "Total Beats :  2929\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 16280.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 114\n",
      "Total Beats :  1876\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 16195.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 112\n",
      "Total Beats :  2540\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 14732.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 117\n",
      "Total Beats :  1536\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 17615.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 222\n",
      "Total Beats :  2406\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 17813.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 212\n",
      "Total Beats :  924\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 19823.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Beats :  2631\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 16096.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Beats :  2450\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 15029.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Beats :  2090\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 15470.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Beats :  2124\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 13657.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Beats :  2747\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 14614.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 232"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Beats :  1383\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 16990.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 109\n",
      "Total Beats :  39\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 16710.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 214\n",
      "Total Beats :  281\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 21403.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 105\n",
      "Total Beats :  2568\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 16755.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 106\n",
      "Total Beats :  2068\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 15917.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 219\n",
      "Total Beats :  2174\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 5949.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 121\n",
      "Total Beats :  1864\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 16785.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 100\n",
      "Total Beats :  2274\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 17371.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 230\n",
      "Total Beats :  2463\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 17625.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 124\n",
      "Total Beats :  62\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 19660.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Beats :  1761\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 15817.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 102\n",
      "Total Beats :  108\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 19828.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 115\n",
      "Total Beats :  1954\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 17776.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 233\n",
      "Total Beats :  3139\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 17090.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 201\n",
      "Total Beats :  1888\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 17605.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 231\n",
      "Total Beats :  328\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 19960.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 101\n",
      "Total Beats :  1864\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 17571.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 207\n",
      "Total Beats :  236\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 18745.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 210\n",
      "Total Beats :  2634\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 16783.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 234\n",
      "Total Beats :  2706\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 17378.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 123\n",
      "Total Beats :  1519\n",
      "Preparing Training Data for R-peaks detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 18545.19it/s]\n"
     ]
    }
   ],
   "source": [
    "training_data = CustomImageDataset('filtered-mit-bih-arrhythmia', training_idx, 'train') \n",
    "train_dataloader = DataLoader(training_data, batch_size=4, shuffle=True)\n",
    "\n",
    "val_data = CustomImageDataset('filtered-mit-bih-arrhythmia',  val_idx, 'train')\n",
    "val_dataloader = DataLoader(val_data, batch_size=4, shuffle=True)\n",
    "\n",
    "# testing_data = CustomImageDataset('filtered-mit-bih-arrhythmia',  test_idx, 'test')\n",
    "# test_dataloader = DataLoader(testing_data, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a1ff2e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Choose 50% of remaining. \n",
    "# norm_idx = np.random.choice(rem_idx, size=int(len(rem_idx)*(1/3)), replace=False)\n",
    "\n",
    "# X_train = np.concatenate((X_train[norm_idx],X_train[idx]))\n",
    "# y_train = np.concatenate((y_train[norm_idx],y_train[idx]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "668c2afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def setup_experiment(title, logdir=\"./logs\"):\n",
    "    experiment_name = \"{}@{}\".format(title, datetime.datetime.now().strftime(\"%d.%m.%Y-%H:%M:%S\"))\n",
    "    writer = SummaryWriter(log_dir=os.path.join(logdir, experiment_name))\n",
    "    best_model_path = f\"{title}.best.pth\"\n",
    "    return writer, experiment_name, best_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "20978929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment name: UNet_1D@28.04.2023-18:42:32\n",
      "Model has 79,153 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "model = UNet_1D()\n",
    "\n",
    "\n",
    "# The learning rate needs to be lower than the default used by Adam, otherwise the learning can be unstable.\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=5e-4)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [20,40,60], gamma=0.5)\n",
    "\n",
    "writer, experiment_name, best_model_path = setup_experiment(model.__class__.__name__, logdir=\"./tb_transf\")\n",
    "print(f\"Experiment name: {experiment_name}\")\n",
    "\n",
    "\n",
    "print(f\"Model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters\")\n",
    "# sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "377d07f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, iterator, optimizer, criterion, phase='train', epoch=0, writer=None):\n",
    "    is_train = (phase == 'train')\n",
    "    if is_train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "\n",
    "    # variables for calculating accuracy\n",
    "    n_predicted = 0\n",
    "    n_true_predicted = 0\n",
    "    \n",
    "    with torch.set_grad_enabled(is_train):\n",
    "        for i, batch in enumerate(iterator):\n",
    "            global_i = len(iterator) * epoch + i\n",
    "            \n",
    "            # unpack batch\n",
    "            inputs, labels = batch\n",
    "#             text, postag = batch.text, batch.postag\n",
    "            \n",
    "            # make prediction\n",
    "            pred = model(inputs)\n",
    "            \n",
    "            \n",
    "            # calculate loss\n",
    "            loss = criterion(pred, labels)\n",
    "            \n",
    "            if is_train:\n",
    "                # make optimization step\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    " \n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # dump epoch metrics to tensorboard\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(f\"loss_epoch/{phase}\", epoch_loss / len(iterator), epoch)\n",
    "\n",
    "        return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0502ed45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 0.789\n",
      "\t Val. Loss: 0.589 \n",
      "Epoch: 02\n",
      "\tTrain Loss: 0.563\n",
      "\t Val. Loss: 0.493 \n",
      "Epoch: 03\n",
      "\tTrain Loss: 0.441\n",
      "\t Val. Loss: 0.404 \n",
      "Epoch: 04\n",
      "\tTrain Loss: 0.359\n",
      "\t Val. Loss: 0.328 \n",
      "Epoch: 05\n",
      "\tTrain Loss: 0.294\n",
      "\t Val. Loss: 0.249 \n",
      "Epoch: 06\n",
      "\tTrain Loss: 0.247\n",
      "\t Val. Loss: 0.198 \n",
      "Epoch: 07\n",
      "\tTrain Loss: 0.209\n",
      "\t Val. Loss: 0.167 \n",
      "Epoch: 08\n",
      "\tTrain Loss: 0.179\n",
      "\t Val. Loss: 0.148 \n",
      "Epoch: 09\n",
      "\tTrain Loss: 0.163\n",
      "\t Val. Loss: 0.134 \n",
      "Epoch: 10\n",
      "\tTrain Loss: 0.146\n",
      "\t Val. Loss: 0.114 \n",
      "Epoch: 11\n",
      "\tTrain Loss: 0.124\n",
      "\t Val. Loss: 0.104 \n",
      "Epoch: 12\n",
      "\tTrain Loss: 0.107\n",
      "\t Val. Loss: 0.089 \n",
      "Epoch: 13\n",
      "\tTrain Loss: 0.099\n",
      "\t Val. Loss: 0.084 \n",
      "Epoch: 14\n",
      "\tTrain Loss: 0.084\n",
      "\t Val. Loss: 0.067 \n",
      "Epoch: 15\n",
      "\tTrain Loss: 0.081\n",
      "\t Val. Loss: 0.068 \n",
      "Epoch: 16\n",
      "\tTrain Loss: 0.075\n",
      "\t Val. Loss: 0.059 \n",
      "Epoch: 17\n",
      "\tTrain Loss: 0.067\n",
      "\t Val. Loss: 0.052 \n",
      "Epoch: 18\n",
      "\tTrain Loss: 0.059\n",
      "\t Val. Loss: 0.047 \n",
      "Epoch: 19\n",
      "\tTrain Loss: 0.058\n",
      "\t Val. Loss: 0.045 \n",
      "Epoch: 20\n",
      "\tTrain Loss: 0.053\n",
      "\t Val. Loss: 0.041 \n",
      "Epoch: 21\n",
      "\tTrain Loss: 0.051\n",
      "\t Val. Loss: 0.039 \n",
      "Epoch: 22\n",
      "\tTrain Loss: 0.043\n",
      "\t Val. Loss: 0.033 \n",
      "Epoch: 23\n",
      "\tTrain Loss: 0.042\n",
      "\t Val. Loss: 0.033 \n",
      "Epoch: 24\n",
      "\tTrain Loss: 0.044\n",
      "\t Val. Loss: 0.030 \n",
      "Epoch: 25\n",
      "\tTrain Loss: 0.037\n",
      "\t Val. Loss: 0.029 \n",
      "Epoch: 26\n",
      "\tTrain Loss: 0.036\n",
      "\t Val. Loss: 0.027 \n",
      "Epoch: 27\n",
      "\tTrain Loss: 0.032\n",
      "\t Val. Loss: 0.026 \n",
      "Epoch: 28\n",
      "\tTrain Loss: 0.032\n",
      "\t Val. Loss: 0.023 \n",
      "Epoch: 29\n",
      "\tTrain Loss: 0.031\n",
      "\t Val. Loss: 0.023 \n",
      "Epoch: 30\n",
      "\tTrain Loss: 0.028\n",
      "\t Val. Loss: 0.021 \n",
      "Epoch: 31\n",
      "\tTrain Loss: 0.027\n",
      "\t Val. Loss: 0.021 \n",
      "Epoch: 32\n",
      "\tTrain Loss: 0.028\n",
      "\t Val. Loss: 0.020 \n",
      "Epoch: 33\n",
      "\tTrain Loss: 0.027\n",
      "\t Val. Loss: 0.020 \n",
      "Epoch: 34\n",
      "\tTrain Loss: 0.029\n",
      "\t Val. Loss: 0.019 \n",
      "Epoch: 35\n",
      "\tTrain Loss: 0.025\n",
      "\t Val. Loss: 0.018 \n",
      "Epoch: 36\n",
      "\tTrain Loss: 0.025\n",
      "\t Val. Loss: 0.017 \n",
      "Epoch: 37\n",
      "\tTrain Loss: 0.022\n",
      "\t Val. Loss: 0.018 \n",
      "Epoch: 38\n",
      "\tTrain Loss: 0.022\n",
      "\t Val. Loss: 0.017 \n",
      "Epoch: 39\n",
      "\tTrain Loss: 0.021\n",
      "\t Val. Loss: 0.015 \n",
      "Epoch: 40\n",
      "\tTrain Loss: 0.021\n",
      "\t Val. Loss: 0.016 \n",
      "Epoch: 41\n",
      "\tTrain Loss: 0.021\n",
      "\t Val. Loss: 0.015 \n",
      "Epoch: 42\n",
      "\tTrain Loss: 0.020\n",
      "\t Val. Loss: 0.014 \n",
      "Epoch: 43\n",
      "\tTrain Loss: 0.018\n",
      "\t Val. Loss: 0.015 \n",
      "Epoch: 44\n",
      "\tTrain Loss: 0.017\n",
      "\t Val. Loss: 0.014 \n",
      "Epoch: 45\n",
      "\tTrain Loss: 0.017\n",
      "\t Val. Loss: 0.013 \n",
      "Epoch: 46\n",
      "\tTrain Loss: 0.016\n",
      "\t Val. Loss: 0.013 \n",
      "Epoch: 47\n",
      "\tTrain Loss: 0.015\n",
      "\t Val. Loss: 0.012 \n",
      "Epoch: 48\n",
      "\tTrain Loss: 0.014\n",
      "\t Val. Loss: 0.013 \n",
      "Epoch: 49\n",
      "\tTrain Loss: 0.018\n",
      "\t Val. Loss: 0.013 \n",
      "Epoch: 50\n",
      "\tTrain Loss: 0.014\n",
      "\t Val. Loss: 0.012 \n",
      "Epoch: 51\n",
      "\tTrain Loss: 0.013\n",
      "\t Val. Loss: 0.013 \n",
      "Epoch: 52\n",
      "\tTrain Loss: 0.013\n",
      "\t Val. Loss: 0.014 \n",
      "Epoch: 53\n",
      "\tTrain Loss: 0.012\n",
      "\t Val. Loss: 0.012 \n",
      "Epoch: 54\n",
      "\tTrain Loss: 0.011\n",
      "\t Val. Loss: 0.012 \n",
      "Epoch: 55\n",
      "\tTrain Loss: 0.011\n",
      "\t Val. Loss: 0.010 \n",
      "Epoch: 56\n",
      "\tTrain Loss: 0.010\n",
      "\t Val. Loss: 0.010 \n",
      "Epoch: 57\n",
      "\tTrain Loss: 0.010\n",
      "\t Val. Loss: 0.010 \n",
      "Epoch: 58\n",
      "\tTrain Loss: 0.009\n",
      "\t Val. Loss: 0.011 \n",
      "Epoch: 59\n",
      "\tTrain Loss: 0.009\n",
      "\t Val. Loss: 0.011 \n",
      "Epoch: 60\n",
      "\tTrain Loss: 0.011\n",
      "\t Val. Loss: 0.011 \n",
      "Epoch: 61\n",
      "\tTrain Loss: 0.011\n",
      "\t Val. Loss: 0.011 \n",
      "Epoch: 62\n",
      "\tTrain Loss: 0.009\n",
      "\t Val. Loss: 0.008 \n",
      "Epoch: 63\n",
      "\tTrain Loss: 0.008\n",
      "\t Val. Loss: 0.010 \n",
      "Epoch: 64\n",
      "\tTrain Loss: 0.008\n",
      "\t Val. Loss: 0.010 \n",
      "Epoch: 65\n",
      "\tTrain Loss: 0.008\n",
      "\t Val. Loss: 0.010 \n",
      "Epoch: 66\n",
      "\tTrain Loss: 0.008\n",
      "\t Val. Loss: 0.008 \n",
      "Epoch: 67\n",
      "\tTrain Loss: 0.008\n",
      "\t Val. Loss: 0.011 \n",
      "Epoch: 68\n",
      "\tTrain Loss: 0.007\n",
      "\t Val. Loss: 0.010 \n",
      "Epoch: 69\n",
      "\tTrain Loss: 0.007\n",
      "\t Val. Loss: 0.009 \n",
      "Epoch: 70\n",
      "\tTrain Loss: 0.008\n",
      "\t Val. Loss: 0.009 \n",
      "Epoch: 71\n",
      "\tTrain Loss: 0.007\n",
      "\t Val. Loss: 0.008 \n",
      "Epoch: 72\n",
      "\tTrain Loss: 0.006\n",
      "\t Val. Loss: 0.010 \n",
      "Epoch: 73\n",
      "\tTrain Loss: 0.006\n",
      "\t Val. Loss: 0.009 \n",
      "Epoch: 74\n",
      "\tTrain Loss: 0.006\n",
      "\t Val. Loss: 0.010 \n",
      "Epoch: 75\n",
      "\tTrain Loss: 0.006\n",
      "\t Val. Loss: 0.009 \n",
      "Epoch: 76\n",
      "\tTrain Loss: 0.005\n",
      "\t Val. Loss: 0.009 \n",
      "Epoch: 77\n",
      "\tTrain Loss: 0.007\n",
      "\t Val. Loss: 0.010 \n",
      "Epoch: 78\n",
      "\tTrain Loss: 0.005\n",
      "\t Val. Loss: 0.008 \n",
      "Epoch: 79\n",
      "\tTrain Loss: 0.006\n",
      "\t Val. Loss: 0.008 \n",
      "Epoch: 80\n",
      "\tTrain Loss: 0.007\n",
      "\t Val. Loss: 0.008 \n",
      "Epoch: 81\n",
      "\tTrain Loss: 0.005\n",
      "\t Val. Loss: 0.008 \n",
      "Epoch: 82\n",
      "\tTrain Loss: 0.007\n",
      "\t Val. Loss: 0.008 \n",
      "Epoch: 83\n",
      "\tTrain Loss: 0.005\n",
      "\t Val. Loss: 0.009 \n",
      "Epoch: 84\n",
      "\tTrain Loss: 0.007\n",
      "\t Val. Loss: 0.008 \n",
      "Epoch: 85\n",
      "\tTrain Loss: 0.004\n",
      "\t Val. Loss: 0.009 \n",
      "Epoch: 86\n",
      "\tTrain Loss: 0.005\n",
      "\t Val. Loss: 0.008 \n",
      "Epoch: 87\n",
      "\tTrain Loss: 0.004\n",
      "\t Val. Loss: 0.008 \n",
      "Epoch: 88\n",
      "\tTrain Loss: 0.005\n",
      "\t Val. Loss: 0.009 \n",
      "Epoch: 89\n",
      "\tTrain Loss: 0.004\n",
      "\t Val. Loss: 0.008 \n",
      "Epoch: 90\n",
      "\tTrain Loss: 0.005\n",
      "\t Val. Loss: 0.007 \n",
      "Epoch: 91\n",
      "\tTrain Loss: 0.004\n",
      "\t Val. Loss: 0.007 \n",
      "Epoch: 92\n",
      "\tTrain Loss: 0.003\n",
      "\t Val. Loss: 0.007 \n",
      "Epoch: 93\n",
      "\tTrain Loss: 0.003\n",
      "\t Val. Loss: 0.010 \n",
      "Epoch: 94\n",
      "\tTrain Loss: 0.005\n",
      "\t Val. Loss: 0.007 \n",
      "Epoch: 95\n",
      "\tTrain Loss: 0.004\n",
      "\t Val. Loss: 0.007 \n",
      "Epoch: 96\n",
      "\tTrain Loss: 0.003\n",
      "\t Val. Loss: 0.008 \n",
      "Epoch: 97\n",
      "\tTrain Loss: 0.003\n",
      "\t Val. Loss: 0.007 \n",
      "Epoch: 98\n",
      "\tTrain Loss: 0.004\n",
      "\t Val. Loss: 0.007 \n",
      "Epoch: 99\n",
      "\tTrain Loss: 0.004\n",
      "\t Val. Loss: 0.008 \n",
      "Epoch: 100\n",
      "\tTrain Loss: 0.004\n",
      "\t Val. Loss: 0.008 \n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "log = 'logs'\n",
    "if not os.path.exists(log):\n",
    "    os.makedirs(log)\n",
    "best_model_path = f\"{log}/1d_unet_non_filtered.best.pth\"\n",
    "    \n",
    "best_val_loss = float('+inf')\n",
    "for epoch in range(n_epochs):    \n",
    "    train_loss = run_epoch(model, train_dataloader, optimizer, criterion, phase='train', epoch=epoch, writer = writer)\n",
    "    val_loss = run_epoch(model, val_dataloader, None, criterion, phase='val', epoch=epoch, writer=writer)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
    "    print(f'\\t Val. Loss: {val_loss:.3f} ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c9eb8a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stats(r_ref, r_ans, thr , fs):\n",
    "\n",
    "\n",
    "    FP = 0\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "    for j in range(len(r_ref)):\n",
    "        loc = np.where(np.abs(r_ans - r_ref[j]) <= thr*fs)[0]\n",
    "            \n",
    "        if len(loc) >= 1:\n",
    "            TP += 1\n",
    "            FP += len(loc) - 1\n",
    "        elif len(loc) == 0:\n",
    "            FN += 1\n",
    "\n",
    "    Recall = (TP / (FN + TP))*100\n",
    "    Precision = (TP / (FP + TP))*100\n",
    "\n",
    "    if Recall + Precision == 0:\n",
    "        F1_score = 0\n",
    "    else:\n",
    "        F1_score = (2 * Recall * Precision / (Recall + Precision))\n",
    "    print(\"Recall:{}, Precision(FNR):{}, F1-Score:{}\".format(Recall,Precision,F1_score))\n",
    "    print(\"Total {}\".format(len(r_ref)))\n",
    "    return TP, FN, FP, Precision, Recall, F1_score\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a2cae9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ['TENSORBOARD_BINARY'] = './tb_trans'\n",
    "\n",
    "# %reload_ext tensorboard\n",
    "# logs_base_dir = \"./tb_trans\"\n",
    "# %tensorboard --logdir {logs_base_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4c7eb2d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data for Patient : 122\n",
      "Total Beats :  2477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 71080/71080 [00:05<00:00, 13057.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:100.0, Precision(FNR):34.633668903803134, F1-Score:51.44874857202201\n",
      "Total 2477\n",
      "Loading Data for Patient : 203\n",
      "Total Beats :  3018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 62893/62893 [00:04<00:00, 12911.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:100.0, Precision(FNR):40.24536604880651, F1-Score:57.39279262147\n",
      "Total 3018\n",
      "Loading Data for Patient : 209\n",
      "Total Beats :  3026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 64622/64622 [00:04<00:00, 13273.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:100.0, Precision(FNR):32.13678844519966, F1-Score:48.641697476289984\n",
      "Total 3026\n",
      "Loading Data for Patient : 220\n",
      "Total Beats :  2065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 69987/69987 [00:05<00:00, 12525.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:100.0, Precision(FNR):28.25283896565878, F1-Score:44.05803285683807\n",
      "Total 2065\n",
      "Loading Data for Patient : 228\n",
      "Total Beats :  2094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 56068/56068 [00:04<00:00, 13177.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:100.0, Precision(FNR):31.068249258160236, F1-Score:47.40774281186326\n",
      "Total 2094\n",
      "Loading Data for Patient : 215\n",
      "Total Beats :  3367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 62867/62867 [00:04<00:00, 13488.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:100.0, Precision(FNR):31.899573661771676, F1-Score:48.36948714265192\n",
      "Total 3367\n",
      "Loading Data for Patient : 103\n",
      "Total Beats :  2085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 62153/62153 [00:04<00:00, 13303.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:100.0, Precision(FNR):29.845405095906095, F1-Score:45.9706757799581\n",
      "Total 2085\n",
      "Loading Data for Patient : 116\n",
      "Total Beats :  2413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 65036/65036 [00:04<00:00, 13244.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:100.0, Precision(FNR):33.4164243179615, F1-Score:50.0934191405439\n",
      "Total 2413\n"
     ]
    }
   ],
   "source": [
    "# custom evaluation \n",
    "stats_R = []\n",
    "\n",
    "win_size = 7200\n",
    "stride = 7200//2\n",
    "model_path =  f\"{log}/1d_unet.best.pth\"\n",
    "\n",
    "model = UNet_1D()\n",
    "\n",
    "torch.load(model_path)\n",
    "\n",
    "precision = []\n",
    "recall = []\n",
    "f1_score = []\n",
    "tp_all = []\n",
    "fn_all = []\n",
    "fp_all = []\n",
    "for i, pat_num in zip(range(len(test_idx)), test_idx):\n",
    "# for i, pat_num in zip(range(len(patients)), patients):\n",
    "\n",
    "    \n",
    "    ecg, Rpeaks_pos = load_patient(data_dir, str(pat_num))\n",
    "\n",
    "    padded_indices, data_windows = extract_test_windows(ecg, win_size, stride)\n",
    "    data_windows = np.transpose(data_windows, (0, 2, 1))\n",
    "\n",
    "    X_inp = torch.from_numpy(data_windows).float()\n",
    "    \n",
    "    half_pad = (7232 - 20*360) // 2\n",
    "    p = torch.nn.ConstantPad1d(half_pad, 0)\n",
    "    data_windows = p(X_inp)\n",
    "\n",
    "    predictions = model(data_windows)[:, :, 16:-16].detach().numpy()\n",
    "    predictions = np.transpose(predictions, (0, 2, 1))\n",
    "    \n",
    "    predictions = mean(win_idx=padded_indices, preds=predictions, \n",
    "                           orig_len=ecg.shape[0],win_size=win_size,\n",
    "                           stride= stride)\n",
    "    assert(predictions.shape == ecg.shape)\n",
    "    \n",
    "    threshold = 0.5\n",
    "    above_thresh = predictions[predictions > threshold]\n",
    "    above_threshold_idx = np.where(predictions > threshold)[0]\n",
    "    \n",
    "    correct_up = processing.correct_peaks(sig=ecg,\n",
    "                                          peak_inds=above_threshold_idx,\n",
    "                                          search_radius=30,\n",
    "                                          smooth_window_size=30,\n",
    "                                          peak_dir='up')\n",
    "    \n",
    "    filtered_peaks = []\n",
    "    filtered_probs = []\n",
    "\n",
    "    for peak_id in tqdm(np.unique(correct_up)):\n",
    "\n",
    "        points_in_peak = np.where(correct_up == peak_id)[0]\n",
    "        if points_in_peak.shape[0] >= 5:\n",
    "            filtered_probs.append(above_thresh[points_in_peak].mean())\n",
    "            filtered_peaks.append(peak_id)\n",
    "    \n",
    "\n",
    "    filtered_peaks = np.asarray(filtered_peaks)\n",
    "    filtered_probs = np.asarray(filtered_probs)\n",
    "    \n",
    "    thr = 0.15 # 150 ms\n",
    "    fs = 360\n",
    "    TP, FN, FP, pr_pat, rec_pat, f1_pat = calculate_stats(Rpeaks_pos, filtered_peaks, thr, fs)\n",
    "    tp_all.append(TP)\n",
    "    fn_all.append(FN)\n",
    "    fp_all.append(FP)\n",
    "    \n",
    "    precision.append(pr_pat)\n",
    "    recall.append(rec_pat)\n",
    "    f1_score.append(f1_pat)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e8b4ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20520, 25, 24786, 46.16, 99.89, 62.53)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(tp_all), sum(fn_all), sum(fp_all), round(np.mean(precision), 2), round(np.mean(recall), 2), \\\n",
    "            round(np.mean(f1_score), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "20891bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86017 & 2 & 134595 & 38.79 & 99.98 & 55.61\n"
     ]
    }
   ],
   "source": [
    "# all data set _raw\n",
    "print('{} & {} & {} & {} & {} & {}'.format(sum(tp_all), sum(fn_all), sum(fp_all), round(np.mean(precision), 2), round(np.mean(recall), 2), \\\n",
    "            round(np.mean(f1_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1a9ae1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20545 & 0 & 42333 & 32.69 & 100.0 & 49.17\n"
     ]
    }
   ],
   "source": [
    "# all filtered\n",
    "print('{} & {} & {} & {} & {} & {}'.format(sum(tp_all), sum(fn_all), sum(fp_all), round(np.mean(precision), 2), round(np.mean(recall), 2), \\\n",
    "            round(np.mean(f1_score), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6493b6cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33.179762118786385, 100.0, 49.74963423130878)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(precision), np.mean(recall), np.mean(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7a8325f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41.21190402002332, 99.95071201730943, 58.30041044428638)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtered data \n",
    "np.mean(precision), np.mean(recall), np.mean(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6eef8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc09d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "eab8e0a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38.02923894402076, 99.52015426207396, 54.856202381625394)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtered data \n",
    "np.mean(precision), np.mean(recall), np.mean(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece39d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "913971b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "fedcc008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1             [-1, 16, 3616]             160\n",
      "        Downsample-2             [-1, 16, 3616]               0\n",
      "            Conv1d-3             [-1, 16, 1808]           2,320\n",
      "       BatchNorm1d-4             [-1, 16, 1808]              32\n",
      "        Downsample-5             [-1, 16, 1808]               0\n",
      "            Conv1d-6              [-1, 32, 904]           3,104\n",
      "       BatchNorm1d-7              [-1, 32, 904]              64\n",
      "        Downsample-8              [-1, 32, 904]               0\n",
      "            Conv1d-9              [-1, 32, 452]           6,176\n",
      "      BatchNorm1d-10              [-1, 32, 452]              64\n",
      "       Downsample-11              [-1, 32, 452]               0\n",
      "           Conv1d-12              [-1, 64, 226]           6,208\n",
      "      BatchNorm1d-13              [-1, 64, 226]             128\n",
      "       Downsample-14              [-1, 64, 226]               0\n",
      "           Conv1d-15              [-1, 64, 113]          12,352\n",
      "      BatchNorm1d-16              [-1, 64, 113]             128\n",
      "       Downsample-17              [-1, 64, 113]               0\n",
      "  ConvTranspose1d-18              [-1, 64, 226]          12,352\n",
      "      BatchNorm1d-19              [-1, 64, 226]             128\n",
      "         Upsample-20              [-1, 64, 226]               0\n",
      "  ConvTranspose1d-21              [-1, 32, 452]          12,320\n",
      "      BatchNorm1d-22              [-1, 32, 452]              64\n",
      "         Upsample-23              [-1, 32, 452]               0\n",
      "  ConvTranspose1d-24              [-1, 32, 904]          12,320\n",
      "      BatchNorm1d-25              [-1, 32, 904]              64\n",
      "         Upsample-26              [-1, 32, 904]               0\n",
      "  ConvTranspose1d-27             [-1, 16, 1808]           6,160\n",
      "      BatchNorm1d-28             [-1, 16, 1808]              32\n",
      "         Upsample-29             [-1, 16, 1808]               0\n",
      "  ConvTranspose1d-30             [-1, 16, 3616]           4,624\n",
      "      BatchNorm1d-31             [-1, 16, 3616]              32\n",
      "         Upsample-32             [-1, 16, 3616]               0\n",
      "  ConvTranspose1d-33              [-1, 1, 7232]             289\n",
      "          Sigmoid-34              [-1, 1, 7232]               0\n",
      "  Conv1DTranspose-35              [-1, 1, 7232]               0\n",
      "================================================================\n",
      "Total params: 79,121\n",
      "Trainable params: 79,121\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.03\n",
      "Forward/backward pass size (MB): 6.51\n",
      "Params size (MB): 0.30\n",
      "Estimated Total Size (MB): 6.84\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# summary(model(20*360))\n",
    "\n",
    "summary(model, (1, 7232))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "532b2c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223\n",
      "101\n",
      "208\n",
      "220\n",
      "115\n",
      "219\n",
      "116\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, pat_num in zip(range(len(test_idx)), test_idx):\n",
    "    print(pat_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f8846f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet_1D(\n",
       "  (down_stack): ModuleList(\n",
       "    (0): Downsample(\n",
       "      (conv): Conv1d(1, 16, kernel_size=(9,), stride=(2,), padding=(4,))\n",
       "      (batchnorm): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): Downsample(\n",
       "      (conv): Conv1d(16, 16, kernel_size=(9,), stride=(2,), padding=(4,))\n",
       "      (batchnorm): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): Downsample(\n",
       "      (conv): Conv1d(16, 32, kernel_size=(6,), stride=(2,), padding=(2,))\n",
       "      (batchnorm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (3): Downsample(\n",
       "      (conv): Conv1d(32, 32, kernel_size=(6,), stride=(2,), padding=(2,))\n",
       "      (batchnorm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): Downsample(\n",
       "      (conv): Conv1d(32, 64, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (batchnorm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (5): Downsample(\n",
       "      (conv): Conv1d(64, 64, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (batchnorm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (up_stack): ModuleList(\n",
       "    (0): Upsample(\n",
       "      (conv_transpose): ConvTranspose1d(64, 64, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n",
       "      (batchnorm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (1): Upsample(\n",
       "      (conv_transpose): ConvTranspose1d(128, 32, kernel_size=(3,), stride=(2,), padding=(1,), output_padding=(1,))\n",
       "      (batchnorm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (2): Upsample(\n",
       "      (conv_transpose): ConvTranspose1d(64, 32, kernel_size=(6,), stride=(2,), padding=(2,))\n",
       "      (batchnorm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (3): Upsample(\n",
       "      (conv_transpose): ConvTranspose1d(64, 16, kernel_size=(6,), stride=(2,), padding=(2,))\n",
       "      (batchnorm): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (4): Upsample(\n",
       "      (conv_transpose): ConvTranspose1d(32, 16, kernel_size=(9,), stride=(2,), padding=(4,), output_padding=(1,))\n",
       "      (batchnorm): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (last): Conv1DTranspose(\n",
       "    (conv_transpose): ConvTranspose1d(32, 1, kernel_size=(9,), stride=(2,), padding=(4,), output_padding=(1,))\n",
       "    (activation): Sigmoid()\n",
       "  )\n",
       "  (inputs): Identity()\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc53a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg",
   "language": "python",
   "name": "ecg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
